{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6162ac66-76f8-4a17-85e9-8f6a20b38adb",
   "metadata": {},
   "source": [
    "### Fine-tune FunctionGemma 270M for Mobile Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e833f3-158d-42b9-b94e-36c7eab2b3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "#export HF_TOKEN=hf_token_w_write_access #bash\n",
    "#CUDA_VISIBLE_DEVICES=0 jupyter lab #bash\n",
    "#Accept(ed) license on https://huggingface.co/google/functiongemma-270m-it\n",
    "import os\n",
    "hf_token = os.environ.get(\"HF_TOKEN\") #from google.colab import userdata #colab only\n",
    "from huggingface_hub import login\n",
    "#hf_token = userdata.get('HF_TOKEN') #colab only\n",
    "login(hf_token) # Login into Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ea1afe8-2726-4d09-9b11-66897c364320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "DType:  torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gemma_model = \"google/functiongemma-270m-it\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    gemma_model,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    "    dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
    "\n",
    "print(f\"Device: {base_model.device}\")\n",
    "print(f\"DType:  {base_model.dtype}\")\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6054a40-3e39-4d96-b069-6d121070bf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell finished\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from random import randint\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "#/home/gy/.cache/huggingface/hub/datasets--google--mobile-actions/\n",
    "#and later f models--google--functiongemma-270m-it/\n",
    "data_file = hf_hub_download(repo_id=\"google/mobile-actions\", filename=\"dataset.jsonl\", repo_type=\"dataset\")\n",
    "dataset = load_dataset(\"text\", data_files=data_file, encoding=\"utf-8\")[\"train\"].shuffle()\n",
    "\n",
    "#print(f\"\\n\\033[1mHere's an example from your dataset:\\033[0m \\n{json.dumps(json.loads(dataset[randint(0, len(dataset) - 1)]['text']), indent=2)}\")\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d195309-32c7-49d5-b9f7-88e153854662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5fef20ae014b688b1759590265a53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell finished\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def apply_format(sample):\n",
    "  template_iputs = json.loads(sample['text'])\n",
    "\n",
    "  prompt_and_completion = tokenizer.apply_chat_template(\n",
    "    template_iputs['messages'],\n",
    "    tools=template_iputs['tools'],\n",
    "    tokenize=False,\n",
    "    # add_generation_prompt is False since we don't need model output after all\n",
    "    # messages.\n",
    "    add_generation_prompt=False)\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(\n",
    "    template_iputs['messages'][:-1],\n",
    "    tools=template_iputs['tools'],\n",
    "    tokenize=False,\n",
    "    # add_generation_prompt is True since we would like to include\n",
    "    # \"<start_of_turn>model\" in the prompt, if needed.\n",
    "    add_generation_prompt=True)\n",
    "\n",
    "  completion = prompt_and_completion[len(prompt):]\n",
    "\n",
    "  return {\n",
    "     \"prompt\": prompt,\n",
    "     \"completion\": completion,\n",
    "     \"split\": template_iputs[\"metadata\"],\n",
    "  }\n",
    "\n",
    "processed_dataset = dataset.map(apply_format)\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755693b8-f857-436d-bd9a-52ed4775a46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell finished\n"
     ]
    }
   ],
   "source": [
    "#gue: PRINTs MOVED TO BOTTOM FOR QUICK UN/COMMENT\n",
    "#@title Review the processed dataset\n",
    "\n",
    "longest_example = max(processed_dataset, key=lambda example: len(example['prompt'] + example['completion']))\n",
    "longest_example_token_count = len(tokenizer.tokenize(longest_example['prompt'] + longest_example['completion']))\n",
    "\n",
    "max_token_count = longest_example_token_count + 100\n",
    "\"\"\"\n",
    "print(\"\\033[1mHere's an example from the formatted dataset:\\033[0m\")\n",
    "print(json.dumps(processed_dataset[randint(0, len(processed_dataset) - 1)], indent=2))\n",
    "\n",
    "print(f\"\\n\\033[1mThe longest example length is {len(longest_example['prompt'] + longest_example['completion'])} with {longest_example_token_count} tokens. We need to set the max_length larger than the token count in SFTConfig below.\\033[0m\")\n",
    "print(json.dumps(longest_example, indent=2))\n",
    "\n",
    "print(f\"\\n\\033[1mUsing max_token_count of {max_token_count} (= {longest_example_token_count} + 100) for training below.\\033[0m\")\n",
    "\"\"\"\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1bb8419-bfec-4e25-b64b-12a34b068529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ed3eee186c432ca642170e98deacbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301637fbe13a4bfb92da36ee3235095e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Prepare train and eval dataset.\n",
    "\n",
    "train_dataset = processed_dataset.filter(lambda example: example['split'] == 'train')\n",
    "eval_dataset = processed_dataset.filter(lambda example: example['split'] == 'eval')\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1014592-19ff-4c92-b4c9-eef55e248893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mPrompt:\u001b[0m Schedule a \"team meeting\" tomorrow at 4pm.\n",
      "\n",
      "\u001b[1mBase model output:\u001b[0m I apologize, but I cannot assist with scheduling meetings. My current tools are designed for managing specific functions like phone calls, creating contacts, opening Wi-Fi settings, and sending emails. I cannot access or manage calendar or meeting scheduling capabilities.\n"
     ]
    }
   ],
   "source": [
    "#@title Test with a prompt\n",
    "\n",
    "from transformers import pipeline\n",
    "from random import randint\n",
    "import re\n",
    "\n",
    "# Create a transformers inference pipeline\n",
    "pipe = pipeline(\"text-generation\", model=gemma_model, tokenizer=tokenizer)\n",
    "\n",
    "user_prompt = \"Schedule a \\\"team meeting\\\" tomorrow at 4pm.\"  #@param {type:\"string\"}\n",
    "messages = [\n",
    "    {\"role\": \"developer\", \"content\": \"Current date and time given in YYYY-MM-DDTHH:MM:SS format: 2024-11-15T05:59:00. You are a model that can do function calling with the following functions\"},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "# Reuse the tools from the sample\n",
    "tools = json.loads(dataset[0]['text'])['tools']\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools=tools,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True)\n",
    "\n",
    "print(f\"\\n\\033[1mPrompt:\\033[0m {user_prompt}\")\n",
    "output = pipe(prompt, max_new_tokens=max_token_count)\n",
    "model_output = output[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "print(f\"\\n\\033[1mBase model output:\\033[0m {model_output}\")\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d744e1-6656-41a2-a07f-0c9232b0b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mInput prompt\u001b[0m   : <bos><start_of_turn>developer\n",
      "Current date and time given in YYYY-MM-DDTHH:MM:SS format: 2026-05-28T21:41:28\n",
      "Day of week is Thursday\n",
      "You are a model that can do function calling with the following functions<start_function_declaration>declaration:create_contact{description:<escape>Creates a contact in the phone's contact list.<escape>,parameters:{properties:{email:{description:<escape>The email address of the contact.<escape>,type:<escape>STRING<escape>},first_name:{description:<escape>The first name of the contact.<escape>,type:<escape>STRING<escape>},last_name:{description:<escape>The last name of the contact.<escape>,type:<escape>STRING<escape>},phone_number:{description:<escape>The phone number of the contact.<escape>,type:<escape>STRING<escape>}},required:[<escape>first_name<escape>,<escape>last_name<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration><start_function_declaration>declaration:show_map{description:<escape>Shows a location on the map.<escape>,parameters:{properties:{query:{description:<escape>The location to search for. May be the name of a place, a business, or an address.<escape>,type:<escape>STRING<escape>}},required:[<escape>query<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration><start_function_declaration>declaration:create_calendar_event{description:<escape>Creates a new calendar event.<escape>,parameters:{properties:{datetime:{description:<escape>The date and time of the event in the format YYYY-MM-DDTHH:MM:SS.<escape>,type:<escape>STRING<escape>},title:{description:<escape>The title of the event.<escape>,type:<escape>STRING<escape>}},required:[<escape>title<escape>,<escape>datetime<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration><start_function_declaration>declaration:turn_off_flashlight{description:<escape>Turns the flashlight off.<escape>,parameters:{type:<escape>OBJECT<escape>}}<end_function_declaration><start_function_declaration>declaration:open_wifi_settings{description:<escape>Opens the Wi-Fi settings.<escape>,parameters:{type:<escape>OBJECT<escape>}}<end_function_declaration><start_function_declaration>declaration:turn_on_flashlight{description:<escape>Turns the flashlight on.<escape>,parameters:{type:<escape>OBJECT<escape>}}<end_function_declaration><start_function_declaration>declaration:send_email{description:<escape>Sends an email.<escape>,parameters:{properties:{body:{description:<escape>The body of the email.<escape>,type:<escape>STRING<escape>},subject:{description:<escape>The subject of the email.<escape>,type:<escape>STRING<escape>},to:{description:<escape>The email address of the recipient.<escape>,type:<escape>STRING<escape>}},required:[<escape>to<escape>,<escape>subject<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration><end_of_turn>\n",
      "<start_of_turn>user\n",
      "Can you please open the Wi-Fi settings menu?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "\u001b[1mExpected output\u001b[0m: <start_function_call>call:open_wifi_settings{}<end_function_call><start_function_response>\n",
      "\n",
      "\u001b[1mActual output\u001b[0m  : <start_function_call>call:open_wifi_settings{}<end_function_call><start_function_response>\n",
      "cell finished\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from random import randint\n",
    "import re\n",
    "\n",
    "# Create a transformers inference pipeline\n",
    "pipe = pipeline(\"text-generation\", model=gemma_model, tokenizer=tokenizer)\n",
    "\n",
    "# Select a random sample from the test dataset\n",
    "rand_idx = randint(0, len(train_dataset) - 1)\n",
    "test_sample = train_dataset[rand_idx]\n",
    "\n",
    "input_prompt = test_sample['prompt']\n",
    "expected_output = test_sample['completion']\n",
    "\n",
    "# Generate the output\n",
    "output = pipe(input_prompt, max_new_tokens=max_token_count, skip_special_tokens=False)\n",
    "actual_output = output[0]['generated_text'][len(input_prompt):].strip()\n",
    "\n",
    "print(f\"\\n\\033[1mInput prompt\\033[0m   : {input_prompt}\")\n",
    "print(f\"\\n\\033[1mExpected output\\033[0m: {expected_output}\")\n",
    "print(f\"\\n\\033[1mActual output\\033[0m  : {actual_output}\")\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5db3506-b203-4e9c-a5a8-eed6f253cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/dl/funcgemma-mobile-actions\n",
      "Training configured\n",
      "cell finished\n"
     ]
    }
   ],
   "source": [
    "#gue unnecessary; hard-coded below outside git-enabled dir\n",
    "#from pathlib import Path\n",
    "#print(Path.cwd())\n",
    "#output_dir = str(Path.cwd() / \"outputs\" / \"functiongemma-270m-mobile-actions\") #STR()!\n",
    "#gue end\n",
    "\n",
    "#configure fine-tune\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from trl import SFTConfig\n",
    "\n",
    "output_dir = \"~/dl/funcgemma-mobile-actions\" #gue\n",
    "#output_dir = \"/content/mobile-actions-functiongemma\"  # Where to save your fine-tuned checkpoints\n",
    "print(output_dir) #gue\n",
    "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=output_dir,                            # Directory to save adapters\n",
    "    num_train_epochs=2,                               # Number of training epochs\n",
    "    per_device_train_batch_size=4,                    # Batch size per device during training\n",
    "    gradient_accumulation_steps=8,                    # Gradient accumulation during training\n",
    "    logging_strategy=\"steps\",                         # Log every steps\n",
    "    eval_strategy=\"steps\",                            # Evaluate loss metrics based on steps\n",
    "    eval_steps=50,                                    # Evaluate loss metrics every 50 steps\n",
    "    logging_steps=50,                                 # Log loss metrics every 50 steps\n",
    "    save_strategy=\"steps\",                            # Save checkpoint every #gue: steps ##epoch\n",
    "    save_steps=50, #gue: maybe 10 for 1st test to be able to resume\n",
    "    save_total_limit=3, #gue: keep last 3\n",
    "    save_safetensors=True, #gue resume-friendly\n",
    "    save_on_each_node=False,\n",
    "    learning_rate=1e-5,                               # Learning rate,\n",
    "    lr_scheduler_type=\"cosine\",                       # Cosine scheduler is often better for full FT\n",
    "    max_length=max_token_count,                       # Max sequence length for model and packing of the dataset\n",
    "    gradient_checkpointing=True,                      # Use gradient checkpointing to save memory\n",
    "    packing=False,                                    # Groups multiple samples in the dataset into a single sequence\n",
    "    optim=\"adamw_torch_fused\",                        # Use fused adamw optimizer\n",
    "    bf16=True,                                        # Use bf16 for mixed precision training\n",
    "    completion_only_loss=True,                        # Train on completion only to improve quality\n",
    "    report_to=\"none\"                                  # No reporting.\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    gemma_model,\n",
    "    device_map=\"auto\", #\"none\" to avoid warning model is already on multiple devices\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation='eager')\n",
    "\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"Training configured\")\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b9de679-59cb-4c60-b1f6-ca9c376828a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cff30ee1d74f05a991061acdfae20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/8693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb5c6d0e7564baaa491e89fec7837e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/8693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779bf8ba3d6f42ba9ddc59792c873ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/8693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d18bd291c96456ba6adf629f7c572d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9fc8615963473f9f9e99176044aec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce04164d9a0470cb70a52aacbf8144b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='544' max='544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [544/544 21:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.021908</td>\n",
       "      <td>0.537001</td>\n",
       "      <td>1044237.000000</td>\n",
       "      <td>0.994428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.018073</td>\n",
       "      <td>0.525342</td>\n",
       "      <td>2089474.000000</td>\n",
       "      <td>0.995528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.016733</td>\n",
       "      <td>0.529755</td>\n",
       "      <td>3134537.000000</td>\n",
       "      <td>0.995891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.014855</td>\n",
       "      <td>0.541534</td>\n",
       "      <td>4177330.000000</td>\n",
       "      <td>0.996177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.533594</td>\n",
       "      <td>5221399.000000</td>\n",
       "      <td>0.996226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.014143</td>\n",
       "      <td>0.530153</td>\n",
       "      <td>6261218.000000</td>\n",
       "      <td>0.996302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.014128</td>\n",
       "      <td>0.520362</td>\n",
       "      <td>7307784.000000</td>\n",
       "      <td>0.996314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.014170</td>\n",
       "      <td>0.520124</td>\n",
       "      <td>8349437.000000</td>\n",
       "      <td>0.996446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>0.520182</td>\n",
       "      <td>9389300.000000</td>\n",
       "      <td>0.996498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.014113</td>\n",
       "      <td>0.520105</td>\n",
       "      <td>10435320.000000</td>\n",
       "      <td>0.996481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved to ~/dl/funcgemma-mobile-actions\n"
     ]
    }
   ],
   "source": [
    "#start training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Train and save the fine-tuned model\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Fine-tuned model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389ae31-c781-4ad0-ab9c-f378db1b591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot training results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access the log history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract training / validation loss\n",
    "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
    "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41857c60-6b7e-4b20-b711-a15c340749e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the fine-tuned model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Create Transformers inference pipeline\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "pipe = pipeline(\"text-generation\", model=trained_model, tokenizer=tokenizer)\n",
    "pipe_base = pipeline(\"text-generation\", model=gemma_model, device_map=\"auto\")\n",
    "\n",
    "# Test a prompt\n",
    "user_prompt = \"Schedule a \\\"team meeting\\\" tomorrow at 4pm.\"  #@param {type:\"string\"}\n",
    "messages = [\n",
    "    {\"role\": \"developer\", \"content\": \"Current date and time given in YYYY-MM-DDTHH:MM:SS format: 2024-11-15T05:59:00. You are a model that can do function calling with the following functions\"},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "# Reuse the tools from the sample\n",
    "tools = json.loads(dataset[0]['text'])['tools']\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools=tools,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True)\n",
    "\n",
    "print(f\"\\n\\033[1mPrompt:\\033[0m {prompt}\")\n",
    "output = pipe(prompt, max_new_tokens=max_token_count)\n",
    "output_base = pipe_base(prompt, max_new_tokens=max_token_count)\n",
    "model_output = output[0]['generated_text'][len(prompt):].strip()\n",
    "model_output_base = output_base[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "print(f\"\\n\\033[1mFine-tuned model output:\\033[0m {model_output}\")\n",
    "\n",
    "print(f\"\\n\\033[1mBase model output:\\033[0m       {model_output_base}\")\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d83d2-34cb-437f-a059-919439ba62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the fine-tuned model\n",
    "#@title Helper functions for evaluation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def extract_function_call(model_output):\n",
    "    \"\"\"\n",
    "    Parses a string containing specific function call markers and returns\n",
    "    a list of function call objects. Here is an example of the obejct:\n",
    "\n",
    "    ```\n",
    "    <start_function_call>call:open_map{query:<escape>San Francisco<escape>}<end_function_call>\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The model output string.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing the function calls.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Pattern to extract the full content of a single function call\n",
    "    # Flags: DOTALL allows matching across newlines if necessary\n",
    "    call_pattern = r\"<start_function_call>(.*?)<end_function_call>\"\n",
    "    raw_calls = re.findall(call_pattern, model_output, re.DOTALL)\n",
    "\n",
    "    for raw_call in raw_calls:\n",
    "        # Check if the content starts with 'call:'\n",
    "        if not raw_call.strip().startswith(\"call:\"):\n",
    "            continue\n",
    "\n",
    "        # Extract function name\n",
    "        # Expected format: call:func_name{...}\n",
    "        try:\n",
    "            # Split only on the first brace to separate name and args\n",
    "            pre_brace, args_segment = raw_call.split(\"{\", 1)\n",
    "\n",
    "            function_name = pre_brace.replace(\"call:\", \"\").strip()\n",
    "\n",
    "            # Remove the trailing closing brace '}'\n",
    "            args_content = args_segment.strip()\n",
    "            if args_content.endswith(\"}\"):\n",
    "                args_content = args_content[:-1]\n",
    "\n",
    "            arguments = {}\n",
    "\n",
    "            # Pattern to extract arguments\n",
    "            # Looks for: key:<escape>value<escape>\n",
    "            # The key pattern [^:,]* ensures we don't accidentally eat previous commas\n",
    "            arg_pattern = r\"(?P<key>[^:,]*?):<escape>(?P<value>.*?)<escape>\"\n",
    "\n",
    "            arg_matches = re.finditer(arg_pattern, args_content, re.DOTALL)\n",
    "\n",
    "            for match in arg_matches:\n",
    "                key = match.group(\"key\").strip()\n",
    "                value = match.group(\"value\")\n",
    "                arguments[key] = value\n",
    "\n",
    "            results.append({\n",
    "                \"function\": {\n",
    "                    \"name\": function_name,\n",
    "                    \"arguments\": arguments\n",
    "                }\n",
    "            })\n",
    "\n",
    "        except ValueError:\n",
    "            # Handles cases where syntax might be malformed (e.g., missing '{')\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "def extract_text(model_output):\n",
    "    \"\"\"\n",
    "    Extracts text content and removing the <end_of_turn> marker.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The model output string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    if not model_output or model_output.startswith(\"<start_function_call>\"):\n",
    "        return None\n",
    "    return model_output.replace(\"<end_of_turn>\", \"\").strip()\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "def get_eval_logs(dataset, pipe):\n",
    "  batch_size = 1\n",
    "  logs = []\n",
    "  # Select a random sample from the test dataset\n",
    "  for i, output in enumerate(pipe(KeyDataset(dataset, \"prompt\"), batch_size=batch_size)):\n",
    "    orig_data = dataset[i]['text']\n",
    "    messages = json.loads(orig_data)['messages']\n",
    "    user_message = messages[1]\n",
    "    assistant_first_message = messages[2]\n",
    "    input_prompt = dataset[i]['prompt']\n",
    "    # Generate the output\n",
    "    model_output_only = output[0]['generated_text'][len(input_prompt):].strip()\n",
    "\n",
    "    logs.append(\n",
    "        {\n",
    "            # The original user prompt/query.\n",
    "            \"user\": user_message['content'],\n",
    "\n",
    "            # List of ground truth function call objects.\n",
    "            \"target_fc\": assistant_first_message.get('tool_calls', []),\n",
    "\n",
    "            # Ground truth text response.\n",
    "            \"target_text\": assistant_first_message.get('content'),\n",
    "\n",
    "            # List of model-generated function call objects.\n",
    "            \"output_fc\": extract_function_call(model_output_only),\n",
    "\n",
    "            # Model-generated text response.\n",
    "            \"output_text\": extract_text(model_output_only),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if (i + 1) % batch_size == 0:\n",
    "      print(f\"Eval process: {(i + 1) * 100.0 / len(dataset):.2f}%\")\n",
    "  return logs\n",
    "\n",
    "def get_scored_data_frame(dataset, pipe):\n",
    "  logs = get_eval_logs(dataset, pipe)\n",
    "  logs_df = pd.DataFrame.from_records(logs)\n",
    "\n",
    "  scored = pd.DataFrame()\n",
    "  scored['user'] = logs_df['user']\n",
    "  scored['target_names'] = logs_df['target_fc'].apply(lambda x: [fc['function']['name'] for fc in x])\n",
    "  scored['output_names'] = logs_df['output_fc'].apply(lambda x: [fc['function']['name'] for fc in x])\n",
    "  scored[\"target_arguments\"] = logs_df['target_fc'].apply(lambda x: [dict(sorted(fc['function']['arguments'].items())) for fc in x])\n",
    "  scored[\"output_arguments\"] = logs_df['output_fc'].apply(lambda x: [dict(sorted(fc['function']['arguments'].items())) for fc in x])\n",
    "  scored['target_text'] = logs_df['target_text']\n",
    "  scored['output_text'] = logs_df['output_text']\n",
    "  scored[\"correct_names\"] = scored[\"target_names\"] == scored[\"output_names\"]\n",
    "  scored[\"correct_arguments\"] = scored[\"target_arguments\"] == scored[\"output_arguments\"]\n",
    "  scored[\"correct\"] = scored[\"correct_names\"] & scored[\"correct_arguments\"]\n",
    "\n",
    "  return scored\n",
    "\n",
    "def review(scored):\n",
    "  scored[\"incorrect_names\"] = scored[\"target_names\"] != scored[\"output_names\"]\n",
    "  scored[\"incorrect_arguments\"] = scored[\"target_arguments\"] != scored[\"output_arguments\"]\n",
    "  scored[\"incorrect\"] = scored[\"incorrect_names\"] | scored[\"incorrect_arguments\"]\n",
    "\n",
    "  for index, row in scored[scored[\"incorrect\"]].iterrows():\n",
    "    print(f\"\\033[1mSample #{index} prompt  \\033[0m: {row[\"user\"]}\")\n",
    "    print(f\"\\033[1mSample #{index} expected\\033[0m: {row[\"target_names\"]}, {row[\"target_arguments\"]}\")\n",
    "    print(f\"\\033[1mSample #{index} actual  \\033[0m: {row[\"output_names\"]}, {row[\"output_arguments\"]}\")\n",
    "    print(\"---------------\")\n",
    "\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57ef4c-9625-4626-8190-cb1a2088b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Evaluate the base model\n",
    "\n",
    "base_scored = get_scored_data_frame(\n",
    "    eval_dataset,\n",
    "    pipeline(\"text-generation\", model=gemma_model, device_map=\"auto\", temperature = 0.001),\n",
    ")\n",
    "\n",
    "base_scored\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344f798-dce0-468d-a13e-375a2907fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Evaluate the fine-tuned model\n",
    "\n",
    "from transformers import pipeline\n",
    "from random import randint\n",
    "import re\n",
    "\n",
    "# Create a transformers inference pipeline\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "trained_scored = get_scored_data_frame(\n",
    "    eval_dataset,\n",
    "    pipeline(\"text-generation\", model=trained_model, tokenizer=tokenizer, temperature = 0.001)\n",
    ")\n",
    "\n",
    "trained_scored\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda71083-323b-4cc2-85e0-49f05d23cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Compare the score of the base and fine-tuned models\n",
    "\n",
    "# Optional: save the score in json file\n",
    "trained_scored.to_json('scored_df_20251215_trained.json')\n",
    "base_scored.to_json('scored_df_20251215_base.json')\n",
    "\n",
    "print(f\"\\033[1mBase model score\\033[0m       : {base_scored[\"correct\"].mean()}\")\n",
    "print(f\"\\033[1mFine-tuned model score\\033[0m : {trained_scored[\"correct\"].mean()}\")\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6286c3-2246-4556-941a-6168b9d9b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Review what are not matching\n",
    "review(trained_scored)\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73191d3e-79df-4c6d-81fa-ba7891a30874",
   "metadata": {},
   "source": [
    "### Save your model and upload to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713fe269-7ef3-4011-8b4a-8f3750bc8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard, ModelCardData, whoami\n",
    "\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "#@markdown Name your model\n",
    "model_name = \"mobile-actions\"    #@param {type:\"string\"}\n",
    "\n",
    "username = whoami()['name']\n",
    "hf_repo_id = f\"{username}/functiongemma-270m-it-{model_name}\"\n",
    "\n",
    "repo_url = trained_model.push_to_hub(hf_repo_id, create_repo=True, commit_message=\"Upload model\")\n",
    "tokenizer.push_to_hub(hf_repo_id)\n",
    "\n",
    "card_content = f\"\"\"\n",
    "---\n",
    "base_model: {gemma_model}\n",
    "tags:\n",
    "- function-calling\n",
    "- mobile-actions\n",
    "- gemma\n",
    "---\n",
    "A fine-tuned model based on `{gemma_model}`.\"\"\"\n",
    "card = ModelCard(card_content)\n",
    "\n",
    "card.push_to_hub(hf_repo_id)\n",
    "\n",
    "print(f\"Uploaded to {repo_url}\")\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bab76bf-e760-4d35-a791-029ae77b62d7",
   "metadata": {},
   "source": [
    "### Conversion to .litertlm for on-device deployment\n",
    "#### gue: adaption and (bug) fixes to the cell to run without above cells\n",
    "#### `conda activate functiongemma2` before starting jupyter lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f71b49-a604-4ca4-ad0d-0a7f981ece65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpy7o8wchy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpy7o8wchy/assets\n",
      "W0000 00:00:1767472547.162751   44106 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1767472547.162786   44106 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "I0000 00:00:1767472547.162937   44106 reader.cc:83] Reading SavedModel from: /tmp/tmpy7o8wchy\n",
      "I0000 00:00:1767472547.168155   44106 reader.cc:52] Reading meta graph with tags { serve }\n",
      "I0000 00:00:1767472547.168166   44106 reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpy7o8wchy\n",
      "I0000 00:00:1767472547.202360   44106 loader.cc:236] Restoring SavedModel bundle.\n",
      "I0000 00:00:1767472547.610956   44106 loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpy7o8wchy\n",
      "I0000 00:00:1767472547.664278   44106 loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 501352 microseconds.\n",
      "I0000 00:00:1767472613.944373   44106 flatbuffer_export.cc:4230] Estimated count of arithmetic ops: 50.776 G  ops, equivalently 25.388 G  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell finished\n"
     ]
    }
   ],
   "source": [
    "#conda create -n functiongemma2 python=3.12 -y #for conversion to .litertlm\n",
    "#conda activate functiongemma2\n",
    "#install rest of functiongemma items, then mods from ipynb conversion section / part:\n",
    "##pip uninstall -y tensorflow #not installed in env, maybe in colab\n",
    "#!pip install ai-edge-torch-nightly --force-reinstall #install(ed) in env\n",
    "##!pip install ai-edge-litert-nightly #Requirement already satisfied\n",
    "#pip install sentencepiece #missing\n",
    "\n",
    "import os\n",
    "from ai_edge_torch.generative.examples.gemma3 import gemma3\n",
    "from ai_edge_torch.generative.utilities import converter\n",
    "from ai_edge_torch.generative.utilities.export_config import ExportConfig\n",
    "from ai_edge_torch.generative.layers import kv_cache\n",
    "\n",
    "#gue: moved to top and adapted to prev settings to make the conversion independent from above\n",
    "checkpoint_dir = \"~/dl/funcgemma-mobile-actions/checkpoint-544\" #SUB to output_dir #\"/content/mobile-actions-functiongemma\"\n",
    "\n",
    "litertlm_output_dir = \"~/dl/funcgemma-mobile-actions-litertlm\" #'/content/litertlm'\n",
    "litertlm_output_dir = os.path.expanduser(litertlm_output_dir) #gue: popular expansion problem py v bash?\n",
    "os.makedirs(litertlm_output_dir, exist_ok=True)\n",
    "\n",
    "#gue 2:\n",
    "output_dir = \"~/dl/funcgemma-mobile-actions\"\n",
    "output_dir = os.path.expanduser(output_dir)\n",
    "checkpoint_dir = os.path.expanduser(checkpoint_dir)\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import shutil\n",
    "\n",
    "BASE_MODEL_ID = \"google/functiongemma-270m-it\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=False)\n",
    "spm_src = tok.vocab_file\n",
    "\n",
    "spm_dst = os.path.join(checkpoint_dir, \"tokenizer.model\")\n",
    "shutil.copy(spm_src, spm_dst)\n",
    "\n",
    "assert os.path.isfile(spm_dst)\n",
    "\n",
    "tok.save_pretrained(checkpoint_dir)\n",
    "#gue end\n",
    "\n",
    "# Metadata for FunctionGemma\n",
    "llm_metadata = r\"\"\"start_token: {\n",
    "    token_ids: {\n",
    "        ids: [ 2 ]\n",
    "    }\n",
    "}\n",
    "stop_tokens: {\n",
    "    token_str: \"<end_of_turn>\"\n",
    "}\n",
    "stop_tokens: {\n",
    "    token_str: \"<start_function_response>\"\n",
    "}\n",
    "llm_model_type: {\n",
    "    function_gemma: {}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create the LLM metadata file\n",
    "metadata_path = os.path.join(litertlm_output_dir, 'base_llm_metadata.textproto')\n",
    "with open(metadata_path, 'w') as f:\n",
    "  f.write(llm_metadata)\n",
    "\n",
    "# Import the weights and build the PyTorch model\n",
    "pytorch_model = gemma3.build_model_270m(checkpoint_dir)\n",
    "\n",
    "# Setup the export configurations and parameters for text generation models.\n",
    "export_config = ExportConfig()\n",
    "export_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\n",
    "export_config.mask_as_input = True\n",
    "\n",
    "# Convert to LiteRT-LM Format\n",
    "converter.convert_to_litert(\n",
    "    pytorch_model,\n",
    "    output_path=litertlm_output_dir,\n",
    "    output_name_prefix=\"mobile-actions\",\n",
    "    prefill_seq_len=256,\n",
    "    kv_cache_max_len=1024,\n",
    "    quantize=\"dynamic_int8\",\n",
    "    export_config=export_config,\n",
    "    #tokenizer_model_path=os.path.join(checkpoint_dir, 'tokenizer.model'),\n",
    "    tokenizer_model_path=os.path.join(checkpoint_dir, \"tokenizer.model\"), #gue\n",
    "    base_llm_metadata_path=metadata_path,\n",
    "    output_format=\"litertlm\",\n",
    ")\n",
    "print(\"cell finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a617038f-a2d1-4c9c-aa01-75a39c610213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41ac3a-793e-4a09-b24e-9f6f01d36718",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save the .litertlm on Google Drive #NOT!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
